{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# HW4 Assignment 1 \u2013 Logistic Regression\n\nFit a logistic regression model on the `Lag1`\u2013`Lag5` plus `Volume` predictors and report the accuracy on the 2005 test split. The notebook below keeps everything deterministic and self-contained so it can run without external libraries.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import csv\nimport math\nimport random\nfrom pathlib import Path\nfrom typing import List, Sequence, Tuple\n\nDATA_PATH = Path('smarket.csv')\nFEATURE_NAMES = [f'Lag{i}' for i in range(1, 6)] + ['Volume']\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def load_smarket(path: Path) -> Tuple[List[List[float]], List[int], List[int]]:\n    \"\"\"Return raw features, labels (1 for Up, 0 for Down) and the associated years.\"\"\"\n    features, labels, years = [], [], []\n    with path.open(newline='') as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            years.append(int(row['Year']))\n            feat = [float(row[name]) for name in FEATURE_NAMES]\n            features.append(feat)\n            labels.append(1 if row['Direction'].strip().lower() == 'up' else 0)\n    return features, labels, years\n\nX_all, y_all, years = load_smarket(DATA_PATH)\nX_train = [row for row, year in zip(X_all, years) if year < 2005]\ny_train = [label for label, year in zip(y_all, years) if year < 2005]\nX_test = [row for row, year in zip(X_all, years) if year == 2005]\ny_test = [label for label, year in zip(y_all, years) if year == 2005]\n\nprint(f'Total samples: {len(X_all)} (training: {len(X_train)}, testing: {len(X_test)})')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def compute_scaler(dataset: Sequence[Sequence[float]]) -> Tuple[List[float], List[float]]:\n    n_features = len(dataset[0])\n    means = [sum(row[i] for row in dataset) / len(dataset) for i in range(n_features)]\n    stds: List[float] = []\n    for i in range(n_features):\n        mean_i = means[i]\n        variance = sum((row[i] - mean_i) ** 2 for row in dataset) / len(dataset)\n        stds.append(variance ** 0.5 if variance > 0 else 1.0)\n    return means, stds\n\ndef apply_scaler(dataset: Sequence[Sequence[float]], means: Sequence[float], stds: Sequence[float]) -> List[List[float]]:\n    scaled: List[List[float]] = []\n    for row in dataset:\n        scaled.append([(row[i] - means[i]) / stds[i] for i in range(len(row))])\n    return scaled\n\ndef add_bias(dataset: Sequence[Sequence[float]]) -> List[List[float]]:\n    return [[1.0] + list(row) for row in dataset]\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def sigmoid(z: float) -> float:\n    if z >= 0:\n        ez = math.exp(-z)\n        return 1.0 / (1.0 + ez)\n    ez = math.exp(z)\n    return ez / (1.0 + ez)\n\ndef solve_linear_system(matrix: List[List[float]], vector: List[float]) -> List[float]:\n    n = len(vector)\n    augmented = [row[:] + [vector[i]] for i, row in enumerate(matrix)]\n    for col in range(n):\n        pivot = max(range(col, n), key=lambda r: abs(augmented[r][col]))\n        if abs(augmented[pivot][col]) < 1e-12:\n            raise ValueError('Singular matrix encountered in Newton step')\n        if pivot != col:\n            augmented[col], augmented[pivot] = augmented[pivot], augmented[col]\n        pivot_val = augmented[col][col]\n        augmented[col] = [val / pivot_val for val in augmented[col]]\n        for row in range(n):\n            if row == col:\n                continue\n            factor = augmented[row][col]\n            if factor == 0:\n                continue\n            augmented[row] = [augmented[row][i] - factor * augmented[col][i] for i in range(n + 1)]\n    return [augmented[i][-1] for i in range(n)]\n\ndef train_logistic(features: Sequence[Sequence[float]], labels: Sequence[int], *, l2: float = 1e-3,\n                   max_iter: int = 30, tol: float = 1e-9) -> List[float]:\n    n_features = len(features[0])\n    weights = [0.0] * n_features\n    for _ in range(max_iter):\n        gradient = [0.0] * n_features\n        hessian = [[0.0] * n_features for _ in range(n_features)]\n        for row, target in zip(features, labels):\n            z = sum(w * x for w, x in zip(weights, row))\n            p = sigmoid(z)\n            diff = p - target\n            for j in range(n_features):\n                gradient[j] += diff * row[j]\n            weight = p * (1.0 - p)\n            for i in range(n_features):\n                for j in range(n_features):\n                    hessian[i][j] += weight * row[i] * row[j]\n        for j in range(1, n_features):  # L2 regularization skips the bias term.\n            gradient[j] += l2 * weights[j]\n            hessian[j][j] += l2\n        step = solve_linear_system(hessian, gradient)\n        max_delta = max(abs(delta) for delta in step)\n        weights = [w - delta for w, delta in zip(weights, step)]\n        if max_delta < tol:\n            break\n    return weights\n\ndef predict_probabilities(features: Sequence[Sequence[float]], weights: Sequence[float]) -> List[float]:\n    return [sigmoid(sum(w * x for w, x in zip(weights, row))) for row in features]\n\ndef accuracy_from_probs(probs: Sequence[float], labels: Sequence[int], threshold: float) -> float:\n    correct = 0\n    for prob, target in zip(probs, labels):\n        pred = 1 if prob >= threshold else 0\n        if pred == target:\n            correct += 1\n    return correct / len(labels)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def build_folds(n_samples: int, k: int = 5, seed: int = 13) -> List[Tuple[List[int], List[int]]]:\n    indices = list(range(n_samples))\n    random.Random(seed).shuffle(indices)\n    fold_sizes = [n_samples // k] * k\n    for i in range(n_samples % k):\n        fold_sizes[i] += 1\n    folds: List[Tuple[List[int], List[int]]] = []\n    start = 0\n    for size in fold_sizes:\n        end = start + size\n        val_indices = indices[start:end]\n        train_indices = indices[:start] + indices[end:]\n        folds.append((train_indices, val_indices))\n        start = end\n    return folds\n\ndef cross_validate_threshold_and_l2(features: Sequence[Sequence[float]], labels: Sequence[int], *,\n                                    num_folds: int = 5) -> Tuple[float, float, float]:\n    folds = build_folds(len(features), k=num_folds)\n    l2_values = [0.0, 1e-4, 1e-3, 1e-2, 1e-1]\n    thresholds = [t / 100.0 for t in range(40, 70)]\n    best_accuracy = 0.0\n    best_l2 = l2_values[0]\n    best_threshold = thresholds[0]\n    for l2 in l2_values:\n        oof_probs: List[float] = []\n        oof_labels: List[int] = []\n        for train_ids, val_ids in folds:\n            train_subset = [features[i] for i in train_ids]\n            val_subset = [features[i] for i in val_ids]\n            train_labels = [labels[i] for i in train_ids]\n            val_labels = [labels[i] for i in val_ids]\n            means, stds = compute_scaler(train_subset)\n            train_scaled = add_bias(apply_scaler(train_subset, means, stds))\n            val_scaled = add_bias(apply_scaler(val_subset, means, stds))\n            weights = train_logistic(train_scaled, train_labels, l2=l2)\n            oof_probs.extend(predict_probabilities(val_scaled, weights))\n            oof_labels.extend(val_labels)\n        for threshold in thresholds:\n            acc = accuracy_from_probs(oof_probs, oof_labels, threshold)\n            if acc > best_accuracy:\n                best_accuracy = acc\n                best_l2 = l2\n                best_threshold = threshold\n    return best_accuracy, best_l2, best_threshold\n\ncv_accuracy, best_l2, best_threshold = cross_validate_threshold_and_l2(X_train, y_train)\nprint(f'Cross-validated accuracy: {cv_accuracy:.3f} using l2={best_l2} and threshold={best_threshold:.2f}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "train_means, train_stds = compute_scaler(X_train)\nX_train_scaled = add_bias(apply_scaler(X_train, train_means, train_stds))\nX_test_scaled = add_bias(apply_scaler(X_test, train_means, train_stds))\nfinal_weights = train_logistic(X_train_scaled, y_train, l2=best_l2)\ntest_probabilities = predict_probabilities(X_test_scaled, final_weights)\ntest_accuracy = accuracy_from_probs(test_probabilities, y_test, best_threshold)\nprint(f'Classification accuracy on 2005 test data: {test_accuracy:.3f}')\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}