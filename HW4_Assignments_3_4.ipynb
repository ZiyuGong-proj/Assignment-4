{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HW4 \u2013 Discriminant Analysis (Assignments 3 & 4)\n",
        "\n",
        "This notebook trains Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) classifiers on the `smarket.csv` dataset using only the `Lag1` and `Lag2` predictors, following the HW4 requirements. Training uses records with `Year < 2005` and evaluation uses `Year = 2005`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "try:\n",
        "    from IPython.display import SVG as _SVG, display as _display\n",
        "except ImportError:  # Fallback for headless environments\n",
        "    _SVG = None\n",
        "    def _display(obj):\n",
        "        print(obj)\n",
        "\n",
        "def load_lag_features(csv_path):\n",
        "    years, features, labels = [], [], []\n",
        "    with open(csv_path, newline='') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            years.append(int(row['Year']))\n",
        "            features.append([float(row['Lag1']), float(row['Lag2'])])\n",
        "            labels.append(1 if row['Direction'].strip().lower() == 'up' else 0)\n",
        "    return years, features, labels\n",
        "\n",
        "def split_by_year(years, features, labels, test_year=2005):\n",
        "    X_train, y_train, X_test, y_test = [], [], [], []\n",
        "    for year, feat, target in zip(years, features, labels):\n",
        "        if year < test_year:\n",
        "            X_train.append(feat)\n",
        "            y_train.append(target)\n",
        "        elif year == test_year:\n",
        "            X_test.append(feat)\n",
        "            y_test.append(target)\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "def group_by_class(features, targets):\n",
        "    grouped = defaultdict(list)\n",
        "    for feat, target in zip(features, targets):\n",
        "        grouped[target].append(feat)\n",
        "    return grouped\n",
        "\n",
        "def compute_means(grouped):\n",
        "    means = {}\n",
        "    for cls, pts in grouped.items():\n",
        "        means[cls] = [sum(point[i] for point in pts) / len(pts) for i in range(2)]\n",
        "    return means\n",
        "\n",
        "def invert_2x2(matrix):\n",
        "    det = matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n",
        "    return [[matrix[1][1] / det, -matrix[0][1] / det],\n",
        "            [-matrix[1][0] / det, matrix[0][0] / det]]\n",
        "\n",
        "def fit_lda(features, targets):\n",
        "    grouped = group_by_class(features, targets)\n",
        "    means = compute_means(grouped)\n",
        "    priors = {cls: len(grouped[cls]) / len(targets) for cls in grouped}\n",
        "    pooled = [[0.0, 0.0], [0.0, 0.0]]\n",
        "    for feat, target in zip(features, targets):\n",
        "        diff0 = feat[0] - means[target][0]\n",
        "        diff1 = feat[1] - means[target][1]\n",
        "        pooled[0][0] += diff0 * diff0\n",
        "        pooled[0][1] += diff0 * diff1\n",
        "        pooled[1][0] += diff1 * diff0\n",
        "        pooled[1][1] += diff1 * diff1\n",
        "    denom = len(features) - len(grouped)\n",
        "    pooled = [[value / denom for value in row] for row in pooled]\n",
        "    inv_cov = invert_2x2(pooled)\n",
        "    log_priors = {cls: math.log(prior) for cls, prior in priors.items()}\n",
        "    return {'means': means, 'inv_cov': inv_cov, 'log_priors': log_priors}\n",
        "\n",
        "def lda_predict_prob(model, feat):\n",
        "    means = model['means']\n",
        "    inv_cov = model['inv_cov']\n",
        "    log_priors = model['log_priors']\n",
        "    scores = {}\n",
        "    for cls, mean in means.items():\n",
        "        w0 = inv_cov[0][0] * mean[0] + inv_cov[0][1] * mean[1]\n",
        "        w1 = inv_cov[1][0] * mean[0] + inv_cov[1][1] * mean[1]\n",
        "        linear = w0 * feat[0] + w1 * feat[1]\n",
        "        shrink = 0.5 * (mean[0] * w0 + mean[1] * w1)\n",
        "        scores[cls] = linear - shrink + log_priors[cls]\n",
        "    diff = scores[1] - scores[0]\n",
        "    return 1.0 / (1.0 + math.exp(-diff))\n",
        "\n",
        "def fit_qda(features, targets):\n",
        "    grouped = group_by_class(features, targets)\n",
        "    means = compute_means(grouped)\n",
        "    priors = {cls: len(grouped[cls]) / len(targets) for cls in grouped}\n",
        "    inv_covs = {}\n",
        "    log_dets = {}\n",
        "    for cls, pts in grouped.items():\n",
        "        cov = [[0.0, 0.0], [0.0, 0.0]]\n",
        "        for feat in pts:\n",
        "            diff0 = feat[0] - means[cls][0]\n",
        "            diff1 = feat[1] - means[cls][1]\n",
        "            cov[0][0] += diff0 * diff0\n",
        "            cov[0][1] += diff0 * diff1\n",
        "            cov[1][0] += diff1 * diff0\n",
        "            cov[1][1] += diff1 * diff1\n",
        "        denom = len(pts) - 1\n",
        "        cov = [[value / denom for value in row] for row in cov]\n",
        "        inv_covs[cls] = invert_2x2(cov)\n",
        "        det = cov[0][0] * cov[1][1] - cov[0][1] * cov[1][0]\n",
        "        log_dets[cls] = math.log(det)\n",
        "    log_priors = {cls: math.log(prior) for cls, prior in priors.items()}\n",
        "    return {'means': means, 'inv_covs': inv_covs, 'log_dets': log_dets, 'log_priors': log_priors}\n",
        "\n",
        "def qda_predict_prob(model, feat):\n",
        "    means = model['means']\n",
        "    inv_covs = model['inv_covs']\n",
        "    log_dets = model['log_dets']\n",
        "    log_priors = model['log_priors']\n",
        "    scores = {}\n",
        "    for cls, mean in means.items():\n",
        "        diff0 = feat[0] - mean[0]\n",
        "        diff1 = feat[1] - mean[1]\n",
        "        inv = inv_covs[cls]\n",
        "        quad = diff0 * (inv[0][0] * diff0 + inv[0][1] * diff1) + diff1 * (inv[1][0] * diff0 + inv[1][1] * diff1)\n",
        "        scores[cls] = -0.5 * (log_dets[cls] + quad) + log_priors[cls]\n",
        "    max_score = max(scores.values())\n",
        "    exp_scores = {cls: math.exp(value - max_score) for cls, value in scores.items()}\n",
        "    total = sum(exp_scores.values())\n",
        "    return exp_scores[1] / total\n",
        "\n",
        "def roc_curve_data(y_true, scores):\n",
        "    ranked = sorted(zip(scores, y_true), key=lambda item: item[0], reverse=True)\n",
        "    positives = sum(y_true)\n",
        "    negatives = len(y_true) - positives\n",
        "    tpr = [0.0]\n",
        "    fpr = [0.0]\n",
        "    tp = fp = 0\n",
        "    for score, target in ranked:\n",
        "        if target == 1:\n",
        "            tp += 1\n",
        "        else:\n",
        "            fp += 1\n",
        "        tpr.append(tp / positives if positives else 0.0)\n",
        "        fpr.append(fp / negatives if negatives else 0.0)\n",
        "    tpr.append(1.0)\n",
        "    fpr.append(1.0)\n",
        "    auc = 0.0\n",
        "    for idx in range(len(fpr) - 1):\n",
        "        auc += (fpr[idx + 1] - fpr[idx]) * (tpr[idx] + tpr[idx + 1]) / 2.0\n",
        "    return fpr, tpr, auc\n",
        "\n",
        "def render_roc_svg(fpr, tpr, title):\n",
        "    width = 500\n",
        "    height = 500\n",
        "    margin = 50\n",
        "    plot_width = width - 2 * margin\n",
        "    plot_height = height - 2 * margin\n",
        "    def sx(value):\n",
        "        return margin + value * plot_width\n",
        "    def sy(value):\n",
        "        return height - (margin + value * plot_height)\n",
        "    commands = []\n",
        "    for idx in range(len(fpr)):\n",
        "        prefix = 'M' if idx == 0 else 'L'\n",
        "        commands.append(f\"{prefix} {sx(fpr[idx]):.2f} {sy(tpr[idx]):.2f}\")\n",
        "    diagonal = f\"M {sx(0):.2f} {sy(0):.2f} L {sx(1):.2f} {sy(1):.2f}\"\n",
        "    path_data = ' '.join(commands)\n",
        "    template = (\n",
        "        \"<svg width='{width}' height='{height}' xmlns='http://www.w3.org/2000/svg'>\n",
        "\"\n",
        "        \"<rect width='{width}' height='{height}' fill='white'/>\n",
        "\"\n",
        "        \"<text x='{title_x}' y='30' text-anchor='middle' font-size='16' font-weight='bold'>{title}</text>\n",
        "\"\n",
        "        \"<line x1='{margin}' y1='{bottom}' x2='{right}' y2='{bottom}' stroke='black'/>\n",
        "\"\n",
        "        \"<line x1='{margin}' y1='{bottom}' x2='{margin}' y2='{margin}' stroke='black'/>\n",
        "\"\n",
        "        \"<path d='{diagonal}' stroke='#999' stroke-width='1.5' fill='none' stroke-dasharray='5,5'/>\n",
        "\"\n",
        "        \"<path d='{path_data}' stroke='#16537e' stroke-width='2.5' fill='none'/>\n",
        "\"\n",
        "        \"<text x='{title_x}' y='{bottom_label}' text-anchor='middle' font-size='12'>False Positive Rate</text>\n",
        "\"\n",
        "        \"<text x='15' y='{vertical_mid}' transform='rotate(-90 15,{vertical_mid})' font-size='12'>True Positive Rate</text>\n",
        "\"\n",
        "        \"</svg>\"\n",
        "    )\n",
        "    return template.format(\n",
        "        width=width,\n",
        "        height=height,\n",
        "        title_x=width / 2,\n",
        "        margin=margin,\n",
        "        bottom=height - margin,\n",
        "        right=width - margin,\n",
        "        diagonal=diagonal,\n",
        "        path_data=path_data,\n",
        "        bottom_label=height - 10,\n",
        "        vertical_mid=height / 2\n",
        "    )\n",
        "\n",
        "def show_svg(svg_text, filename):\n",
        "    if _SVG is not None:\n",
        "        _display(_SVG(svg_text))\n",
        "    else:\n",
        "        with open(filename, 'w') as out:\n",
        "            out.write(svg_text)\n",
        "        print(f\"SVG saved to {filename}. Open the file to inspect the ROC curve.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Training samples: 998\nTest samples: 252\n"
        }
      ],
      "source": [
        "years, features, labels = load_lag_features('smarket.csv')\n",
        "X_train, y_train, X_test, y_test = split_by_year(years, features, labels, test_year=2005)\n",
        "print(f'Training samples: {len(X_train)}')\n",
        "print(f'Test samples: {len(X_test)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "LDA AUC: 0.5584\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<svg width='500' height='500' xmlns='http://www.w3.org/2000/svg'>\n    <rect width='500' height='500' fill='white'/>\n    <text x='250.0' y='30' text-anchor='middle' font-size='16' font-weight='bold'>LDA ROC Curve</text>\n    <line x1='50' y1='450' x2='450' y2='450' stroke='black'/>\n    <line x1='50' y1='450' x2='50' y2='50' stroke='black'/>\n    <path d='M 50.00 450.00 L 450.00 50.00' stroke='#999' stroke-width='1.5' fill='none' stroke-dasharray='5,5'/>\n    <path d='M 50.00 450.00 L 50.00 447.16 L 53.60 447.16 L 57.21 447.16 L 60.81 447.16 L 64.41 447.16 L 64.41 444.33 L 64.41 441.49 L 68.02 441.49 L 68.02 438.65 L 68.02 435.82 L 68.02 432.98 L 68.02 430.14 L 68.02 427.30 L 71.62 427.30 L 71.62 424.47 L 75.23 424.47 L 75.23 421.63 L 78.83 421.63 L 82.43 421.63 L 86.04 421.63 L 86.04 418.79 L 86.04 415.96 L 86.04 413.12 L 86.04 410.28 L 86.04 407.45 L 86.04 404.61 L 89.64 404.61 L 89.64 401.77 L 93.24 401.77 L 93.24 398.94 L 93.24 396.10 L 93.24 393.26 L 93.24 390.43 L 93.24 387.59 L 96.85 387.59 L 100.45 387.59 L 104.05 387.59 L 104.05 384.75 L 107.66 384.75 L 107.66 381.91 L 111.26 381.91 L 114.86 381.91 L 114.86 379.08 L 114.86 376.24 L 114.86 373.40 L 118.47 373.40 L 122.07 373.40 L 125.68 373.40 L 125.68 370.57 L 125.68 367.73 L 129.28 367.73 L 132.88 367.73 L 136.49 367.73 L 136.49 364.89 L 136.49 362.06 L 136.49 359.22 L 140.09 359.22 L 140.09 356.38 L 140.09 353.55 L 140.09 350.71 L 140.09 347.87 L 143.69 347.87 L 143.69 345.04 L 143.69 342.20 L 143.69 339.36 L 143.69 336.52 L 147.30 336.52 L 147.30 333.69 L 147.30 330.85 L 150.90 330.85 L 154.50 330.85 L 154.50 328.01 L 158.11 328.01 L 158.11 325.18 L 161.71 325.18 L 165.32 325.18 L 165.32 322.34 L 165.32 319.50 L 165.32 316.67 L 165.32 313.83 L 165.32 310.99 L 165.32 308.16 L 168.92 308.16 L 168.92 305.32 L 168.92 302.48 L 172.52 302.48 L 176.13 302.48 L 176.13 299.65 L 179.73 299.65 L 179.73 296.81 L 179.73 293.97 L 179.73 291.13 L 179.73 288.30 L 179.73 285.46 L 179.73 282.62 L 179.73 279.79 L 179.73 276.95 L 183.33 276.95 L 186.94 276.95 L 190.54 276.95 L 190.54 274.11 L 190.54 271.28 L 194.14 271.28 L 194.14 268.44 L 197.75 268.44 L 197.75 265.60 L 197.75 262.77 L 197.75 259.93 L 201.35 259.93 L 201.35 257.09 L 204.95 257.09 L 204.95 254.26 L 208.56 254.26 L 212.16 254.26 L 215.77 254.26 L 215.77 251.42 L 219.37 251.42 L 219.37 248.58 L 219.37 245.74 L 219.37 242.91 L 219.37 240.07 L 219.37 237.23 L 222.97 237.23 L 222.97 234.40 L 222.97 231.56 L 226.58 231.56 L 230.18 231.56 L 230.18 228.72 L 233.78 228.72 L 233.78 225.89 L 237.39 225.89 L 240.99 225.89 L 240.99 223.05 L 240.99 220.21 L 244.59 220.21 L 248.20 220.21 L 248.20 217.38 L 248.20 214.54 L 248.20 211.70 L 251.80 211.70 L 255.41 211.70 L 255.41 208.87 L 255.41 206.03 L 259.01 206.03 L 262.61 206.03 L 262.61 203.19 L 262.61 200.35 L 266.22 200.35 L 266.22 197.52 L 269.82 197.52 L 269.82 194.68 L 269.82 191.84 L 269.82 189.01 L 269.82 186.17 L 273.42 186.17 L 273.42 183.33 L 273.42 180.50 L 277.03 180.50 L 280.63 180.50 L 284.23 180.50 L 287.84 180.50 L 287.84 177.66 L 287.84 174.82 L 291.44 174.82 L 291.44 171.99 L 291.44 169.15 L 291.44 166.31 L 295.05 166.31 L 295.05 163.48 L 295.05 160.64 L 295.05 157.80 L 295.05 154.96 L 298.65 154.96 L 298.65 152.13 L 302.25 152.13 L 305.86 152.13 L 309.46 152.13 L 309.46 149.29 L 313.06 149.29 L 316.67 149.29 L 320.27 149.29 L 323.87 149.29 L 323.87 146.45 L 327.48 146.45 L 327.48 143.62 L 331.08 143.62 L 331.08 140.78 L 331.08 137.94 L 331.08 135.11 L 331.08 132.27 L 334.68 132.27 L 334.68 129.43 L 334.68 126.60 L 338.29 126.60 L 338.29 123.76 L 338.29 120.92 L 338.29 118.09 L 338.29 115.25 L 341.89 115.25 L 341.89 112.41 L 345.50 112.41 L 345.50 109.57 L 345.50 106.74 L 349.10 106.74 L 352.70 106.74 L 352.70 103.90 L 356.31 103.90 L 356.31 101.06 L 356.31 98.23 L 359.91 98.23 L 363.51 98.23 L 367.12 98.23 L 370.72 98.23 L 374.32 98.23 L 374.32 95.39 L 377.93 95.39 L 381.53 95.39 L 385.14 95.39 L 385.14 92.55 L 388.74 92.55 L 392.34 92.55 L 392.34 89.72 L 395.95 89.72 L 395.95 86.88 L 395.95 84.04 L 399.55 84.04 L 399.55 81.21 L 403.15 81.21 L 406.76 81.21 L 406.76 78.37 L 406.76 75.53 L 410.36 75.53 L 413.96 75.53 L 413.96 72.70 L 417.57 72.70 L 421.17 72.70 L 424.77 72.70 L 424.77 69.86 L 424.77 67.02 L 424.77 64.18 L 428.38 64.18 L 431.98 64.18 L 431.98 61.35 L 431.98 58.51 L 435.59 58.51 L 439.19 58.51 L 439.19 55.67 L 439.19 52.84 L 439.19 50.00 L 442.79 50.00 L 446.40 50.00 L 450.00 50.00 L 450.00 50.00' stroke='#16537e' stroke-width='2.5' fill='none'/>\n    <text x='250.0' y='490' text-anchor='middle' font-size='12'>False Positive Rate</text>\n    <text x='15' y='250.0' transform='rotate(-90 15,250.0)' font-size='12'>True Positive Rate</text>\n</svg>",
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "lda_model = fit_lda(X_train, y_train)\n",
        "lda_probs = [lda_predict_prob(lda_model, feat) for feat in X_test]\n",
        "lda_fpr, lda_tpr, lda_auc = roc_curve_data(y_test, lda_probs)\n",
        "print(f'LDA AUC: {lda_auc:.4f}')\n",
        "lda_svg = render_roc_svg(lda_fpr, lda_tpr, 'LDA ROC Curve')\n",
        "show_svg(lda_svg, 'lda_roc.svg')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "QDA AUC: 0.5620\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<svg width='500' height='500' xmlns='http://www.w3.org/2000/svg'>\n    <rect width='500' height='500' fill='white'/>\n    <text x='250.0' y='30' text-anchor='middle' font-size='16' font-weight='bold'>QDA ROC Curve</text>\n    <line x1='50' y1='450' x2='450' y2='450' stroke='black'/>\n    <line x1='50' y1='450' x2='50' y2='50' stroke='black'/>\n    <path d='M 50.00 450.00 L 450.00 50.00' stroke='#999' stroke-width='1.5' fill='none' stroke-dasharray='5,5'/>\n    <path d='M 50.00 450.00 L 50.00 447.16 L 53.60 447.16 L 57.21 447.16 L 60.81 447.16 L 60.81 444.33 L 64.41 444.33 L 64.41 441.49 L 68.02 441.49 L 68.02 438.65 L 68.02 435.82 L 68.02 432.98 L 68.02 430.14 L 71.62 430.14 L 71.62 427.30 L 75.23 427.30 L 78.83 427.30 L 82.43 427.30 L 82.43 424.47 L 86.04 424.47 L 86.04 421.63 L 86.04 418.79 L 86.04 415.96 L 89.64 415.96 L 89.64 413.12 L 89.64 410.28 L 89.64 407.45 L 89.64 404.61 L 93.24 404.61 L 96.85 404.61 L 96.85 401.77 L 96.85 398.94 L 96.85 396.10 L 96.85 393.26 L 96.85 390.43 L 100.45 390.43 L 100.45 387.59 L 100.45 384.75 L 104.05 384.75 L 107.66 384.75 L 107.66 381.91 L 111.26 381.91 L 111.26 379.08 L 114.86 379.08 L 114.86 376.24 L 118.47 376.24 L 118.47 373.40 L 122.07 373.40 L 122.07 370.57 L 122.07 367.73 L 125.68 367.73 L 125.68 364.89 L 129.28 364.89 L 129.28 362.06 L 132.88 362.06 L 132.88 359.22 L 132.88 356.38 L 132.88 353.55 L 132.88 350.71 L 132.88 347.87 L 132.88 345.04 L 132.88 342.20 L 136.49 342.20 L 136.49 339.36 L 140.09 339.36 L 140.09 336.52 L 143.69 336.52 L 143.69 333.69 L 143.69 330.85 L 147.30 330.85 L 150.90 330.85 L 154.50 330.85 L 158.11 330.85 L 161.71 330.85 L 161.71 328.01 L 161.71 325.18 L 161.71 322.34 L 161.71 319.50 L 161.71 316.67 L 161.71 313.83 L 165.32 313.83 L 165.32 310.99 L 165.32 308.16 L 168.92 308.16 L 168.92 305.32 L 168.92 302.48 L 168.92 299.65 L 168.92 296.81 L 172.52 296.81 L 172.52 293.97 L 176.13 293.97 L 179.73 293.97 L 179.73 291.13 L 179.73 288.30 L 183.33 288.30 L 183.33 285.46 L 183.33 282.62 L 183.33 279.79 L 183.33 276.95 L 186.94 276.95 L 190.54 276.95 L 194.14 276.95 L 197.75 276.95 L 197.75 274.11 L 197.75 271.28 L 197.75 268.44 L 201.35 268.44 L 201.35 265.60 L 204.95 265.60 L 204.95 262.77 L 204.95 259.93 L 208.56 259.93 L 208.56 257.09 L 208.56 254.26 L 212.16 254.26 L 212.16 251.42 L 212.16 248.58 L 212.16 245.74 L 215.77 245.74 L 215.77 242.91 L 215.77 240.07 L 215.77 237.23 L 215.77 234.40 L 215.77 231.56 L 219.37 231.56 L 222.97 231.56 L 226.58 231.56 L 230.18 231.56 L 230.18 228.72 L 233.78 228.72 L 237.39 228.72 L 237.39 225.89 L 240.99 225.89 L 240.99 223.05 L 240.99 220.21 L 244.59 220.21 L 244.59 217.38 L 248.20 217.38 L 248.20 214.54 L 251.80 214.54 L 255.41 214.54 L 259.01 214.54 L 262.61 214.54 L 266.22 214.54 L 266.22 211.70 L 266.22 208.87 L 266.22 206.03 L 269.82 206.03 L 269.82 203.19 L 269.82 200.35 L 269.82 197.52 L 269.82 194.68 L 269.82 191.84 L 273.42 191.84 L 273.42 189.01 L 273.42 186.17 L 273.42 183.33 L 273.42 180.50 L 277.03 180.50 L 280.63 180.50 L 284.23 180.50 L 284.23 177.66 L 287.84 177.66 L 287.84 174.82 L 287.84 171.99 L 287.84 169.15 L 291.44 169.15 L 295.05 169.15 L 295.05 166.31 L 295.05 163.48 L 295.05 160.64 L 298.65 160.64 L 302.25 160.64 L 302.25 157.80 L 302.25 154.96 L 305.86 154.96 L 309.46 154.96 L 309.46 152.13 L 313.06 152.13 L 316.67 152.13 L 320.27 152.13 L 320.27 149.29 L 320.27 146.45 L 320.27 143.62 L 320.27 140.78 L 320.27 137.94 L 320.27 135.11 L 320.27 132.27 L 323.87 132.27 L 323.87 129.43 L 327.48 129.43 L 327.48 126.60 L 327.48 123.76 L 331.08 123.76 L 331.08 120.92 L 334.68 120.92 L 334.68 118.09 L 334.68 115.25 L 338.29 115.25 L 338.29 112.41 L 338.29 109.57 L 338.29 106.74 L 341.89 106.74 L 341.89 103.90 L 345.50 103.90 L 349.10 103.90 L 352.70 103.90 L 356.31 103.90 L 359.91 103.90 L 359.91 101.06 L 363.51 101.06 L 367.12 101.06 L 370.72 101.06 L 370.72 98.23 L 374.32 98.23 L 377.93 98.23 L 377.93 95.39 L 377.93 92.55 L 377.93 89.72 L 377.93 86.88 L 377.93 84.04 L 377.93 81.21 L 381.53 81.21 L 381.53 78.37 L 385.14 78.37 L 385.14 75.53 L 388.74 75.53 L 392.34 75.53 L 395.95 75.53 L 399.55 75.53 L 403.15 75.53 L 406.76 75.53 L 410.36 75.53 L 410.36 72.70 L 413.96 72.70 L 417.57 72.70 L 417.57 69.86 L 417.57 67.02 L 417.57 64.18 L 421.17 64.18 L 424.77 64.18 L 428.38 64.18 L 428.38 61.35 L 431.98 61.35 L 431.98 58.51 L 435.59 58.51 L 435.59 55.67 L 439.19 55.67 L 442.79 55.67 L 446.40 55.67 L 446.40 52.84 L 446.40 50.00 L 450.00 50.00 L 450.00 50.00' stroke='#16537e' stroke-width='2.5' fill='none'/>\n    <text x='250.0' y='490' text-anchor='middle' font-size='12'>False Positive Rate</text>\n    <text x='15' y='250.0' transform='rotate(-90 15,250.0)' font-size='12'>True Positive Rate</text>\n</svg>",
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "qda_model = fit_qda(X_train, y_train)\n",
        "qda_probs = [qda_predict_prob(qda_model, feat) for feat in X_test]\n",
        "qda_fpr, qda_tpr, qda_auc = roc_curve_data(y_test, qda_probs)\n",
        "print(f'QDA AUC: {qda_auc:.4f}')\n",
        "qda_svg = render_roc_svg(qda_fpr, qda_tpr, 'QDA ROC Curve')\n",
        "show_svg(qda_svg, 'qda_roc.svg')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LDA vs. QDA (Lag1 + Lag2)\n",
        "\n",
        "- **LDA:** AUC = 0.5584. The linear decision boundary struggles to separate the noisy classes, yielding only a slight improvement over random guessing.\n",
        "- **QDA:** AUC = 0.5620. Allowing class-specific covariance matrices captures mild curvature in the class distributions, producing a marginally better ROC curve.\n",
        "\n",
        "Although both models exhibit limited predictive power on the 2005 test period, QDA performs slightly better because it can adapt to differing volatility patterns (covariances) between the `Up` and `Down` classes, whereas LDA is constrained to a single shared covariance estimate.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}